{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmedAbdou07/Coursera-Capstone/blob/master/Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install gensim \n",
        "!pip install tabulate\n",
        "!pip install pprintpp"
      ],
      "metadata": {
        "id": "NXMEDLLMuihD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "metadata": {
        "id": "r2bUvlNWuknt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read tweets\n",
        "df=pd.read_csv('/content/stc_care_tweets.csv')"
      ],
      "metadata": {
        "id": "omEiOTXesRb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "wJGzvAp2sVYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check data no of columns and rows\n",
        "df.shape"
      ],
      "metadata": {
        "id": "4HGp-Rv5sWru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# chec data types\n",
        "df.dtypes"
      ],
      "metadata": {
        "id": "Bg-BLuPosdGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checl last 5 tweets\n",
        "df.tail()"
      ],
      "metadata": {
        "id": "ZssoI0iWsgLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "VJJDGwG68PTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove tagged accounts\n",
        "import re\n",
        "def removetags(x):\n",
        "    return re.sub(r'\\@+[\\w]+[\\w]+', ' ', x)\n",
        "\n",
        "df['processed_text']=df['Text'].apply(lambda x : removetags(x))"
      ],
      "metadata": {
        "id": "V5iET0TQnKz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove http links\n",
        "import re\n",
        "def removelinks(x):\n",
        "    return re.sub(r'http\\S+', '', x)\n",
        "\n",
        "df['processed_text']=df['processed_text'].apply(lambda x : removelinks(x))"
      ],
      "metadata": {
        "id": "2kh5YXgTxUDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if we have English words \n",
        "def checkNonArabicChar(x):\n",
        "    if re.match(r'[a-zA-Z]+', x):\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "df['English_content']=df['processed_text'].apply(lambda x : checkNonArabicChar(x))\n"
      ],
      "metadata": {
        "id": "dFlqvBmYdb7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['English_content'].value_counts()"
      ],
      "metadata": {
        "id": "NrEKKQXihHAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def removestopwords(x):\n",
        "  sw = stopwords.words('arabic')\n",
        "  tokens = nltk.word_tokenize(x)\n",
        "  stopped_tokens = [i for i in tokens if not i in sw]\n",
        "  s=''\n",
        "  for i in stopped_tokens:\n",
        "    s=s+' '+i\n",
        "  return s\n",
        "\n",
        "df['processed_text']=df['processed_text'].apply(lambda x : removestopwords(x))"
      ],
      "metadata": {
        "id": "UogHAA8smQei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet_length']=df['processed_text'].apply(lambda x : len(x))"
      ],
      "metadata": {
        "id": "JFR3FHy0hfZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "ija2pwxGpr1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(36,12))\n",
        "df['tweet_length'].value_counts().plot(kind = 'bar', ylabel = 'frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U3gjelX-jXs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(36,12))\n",
        "df['tweet_length'].value_counts().hist()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GQT5UcS0lMfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['tweet_length'].describe()"
      ],
      "metadata": {
        "id": "DWCvqyrtVq3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize tweets \n",
        "df['processed_text']=df['Text'].apply(lambda x : preprocess(x))"
      ],
      "metadata": {
        "id": "MJNPN7bMs2bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "HwHgu6Prtrbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "U0_1-ERg3D0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Annotation  xlm-Roberta"
      ],
      "metadata": {
        "id": "CophsWXvyzit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize our xlm-roberata model\n",
        "CUDA = True # set to true if using GPU (Runtime -> Change runtime Type -> GPU)\n",
        "BATCH_SIZE = 32\n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "config = AutoConfig.from_pretrained(MODEL) # used for id to label name\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "if CUDA:\n",
        "  model = model.to('cuda')\n",
        "_ = model.eval()"
      ],
      "metadata": {
        "id": "qGQJiv4Bt8dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to tokenize tweets and words\n",
        "def preprocess(corpus):\n",
        "  outcorpus = []\n",
        "  for text in corpus:\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    new_text = \" \".join(new_text)\n",
        "    outcorpus.append(new_text)\n",
        "  return outcorpus"
      ],
      "metadata": {
        "id": "Jxrs2WtwslNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to tokenize tweets and run xlmroberata model on it\n",
        "def forward(text, cuda=True):\n",
        "  text = preprocess(text)\n",
        "  encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "  if cuda:\n",
        "    encoded_input.to('cuda')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().cpu().numpy()\n",
        "  else:\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().numpy()\n",
        "  \n",
        "  scores = softmax(scores, axis=-1)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "Sv96Nv2QuV3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset from tweets only\n",
        "dataset=df['processed_text']"
      ],
      "metadata": {
        "id": "d8mG1ob7vyuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run the model and predict sentiment for each tweet\n",
        "dl = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "all_preds = []\n",
        "for idx,batch in enumerate(dl):\n",
        "  print('Batch ',idx+1,' of ',len(dl))\n",
        "  text = preprocess(batch)\n",
        "  scores = forward(text, cuda=CUDA)\n",
        "  preds = np.argmax(scores, axis=-1)\n",
        "  all_preds.extend(preds)"
      ],
      "metadata": {
        "id": "f_lScJVDvhFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample of tweets along with their annotated sentiment\n",
        "for example in [0,1, 2, 3, 4, 22, 870,1740,2610,3480,4350,5220,6090, 10000]:\n",
        "  pred = all_preds[example]\n",
        "  print(dataset[example], '--->', config.id2label[pred])"
      ],
      "metadata": {
        "id": "vKrXzTKSwCKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataset include original tweets and sentiment \n",
        "tweets=[]\n",
        "sentiment=[]\n",
        "for example in range(10001):\n",
        "  pred = all_preds[example]\n",
        "  tweets.append(dataset[example])\n",
        "  sentiment.append(config.id2label[pred])\n",
        "tweets_dict={'tweets':tweets, 'sentiment':sentiment}\n",
        "tweets_df=pd.DataFrame(tweets_dict)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "YRkBE4y7l_j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "8G2td_XtOF5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=pd.concat([df, tweets_df], axis=1)\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "XKTCMn5syEPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.tail()"
      ],
      "metadata": {
        "id": "-p4DYi-tyW09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I_9FMsA4zEZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reprocessing Part 2"
      ],
      "metadata": {
        "id": "zOq68zDXzEtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove unnecessary spaces\n",
        "def removeUnnecessarySpaces(text):\n",
        "    return re.sub(r'[\\n\\t\\ ]+', ' ', text)\n",
        "\n",
        "tweets['processed_text']=tweets['processed_text'].apply(lambda x : removeUnnecessarySpaces(x))"
      ],
      "metadata": {
        "id": "P0JycaIIqzev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove emojis\n",
        "\n",
        "def remove_emoji(string):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U00002702-\\U000027B0\"\n",
        "                               u\"\\U000024C2-\\U0001F251\"\n",
        "                               u\"\\U0001f926-\\U0001f937\"\n",
        "                               u\"\\U00010000-\\U0010ffff\"\n",
        "                               u\"\\u2640-\\u2642\"\n",
        "                               u\"\\u2600-\\u2B55\"\n",
        "                               u\"\\u200d\"\n",
        "                               u\"\\u23cf\"\n",
        "                               u\"\\u23e9\"\n",
        "                               u\"\\u231a\"\n",
        "                               u\"\\ufe0f\"  # dingbats\n",
        "                               u\"\\u3030\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', string)\n",
        "tweets['processed_text']=tweets['processed_text'].apply(lambda x : remove_emoji(x))"
      ],
      "metadata": {
        "id": "_ESZLWrsytiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets[tweets['tweet_length']==2]"
      ],
      "metadata": {
        "id": "g19PzZVux9A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "from nltk.stem.isri import ISRIStemmer\n",
        "st = ISRIStemmer()\n",
        "\n",
        "tweets['stemmed_text']=tweets['processed_text'].apply(lambda x : st.suf32(x))"
      ],
      "metadata": {
        "id": "0Atzu3HM1j5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# normalization\n",
        "!pip install PyArabic\n",
        "import pyarabic.araby as araby\n",
        "def normalizeArabic(text):\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[إأٱآا]\", \"ا\", text)\n",
        "    text = re.sub(\"ى\", \"ي\", text)\n",
        "    text = re.sub(\"ؤ\", \"ء\", text)\n",
        "    text = re.sub(\"ئ\", \"ء\", text)\n",
        "    text = re.sub(\"ة\", \"ه\", text)\n",
        "    noise = re.compile(\"\"\" ّ    | # Tashdid\n",
        "                             َ    | # Fatha\n",
        "                             ً    | # Tanwin Fath\n",
        "                             ُ    | # Damma\n",
        "                             ٌ    | # Tanwin Damm\n",
        "                             ِ    | # Kasra\n",
        "                             ٍ    | # Tanwin Kasr\n",
        "                             ْ    | # Sukun\n",
        "                             ـ     # Tatwil/Kashida\n",
        "                         \"\"\", re.VERBOSE)\n",
        "    text = re.sub(noise, '', text)\n",
        "    text = re.sub(r'(.)\\1+', r\"\\1\\1\", text) # Remove longation\n",
        "    return araby.strip_tashkeel(text)\n",
        "  \n",
        "tweets['normalized_text']=tweets['stemmed_text'].apply(lambda x : normalizeArabic(x))"
      ],
      "metadata": {
        "id": "JiGGtiU05229"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove non arabic characters\n",
        "def removeNonArabicChar(text):\n",
        "    return re.sub(r'[^0-9\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD.0-9]+', ' ', text)\n",
        "\n",
        "tweets['Arabic_processed_tweets']=tweets['normalized_text'].apply(lambda x : removeNonArabicChar(x))\n",
        "\n",
        "# remove unnecessary spaces\n",
        "def removeUnnecessarySpaces(text):\n",
        "    return re.sub(r'[\\n\\t\\ ]+', ' ', text)\n",
        "\n",
        "tweets['Arabic_processed_tweets']=tweets['Arabic_processed_tweets'].apply(lambda x : removeUnnecessarySpaces(x))"
      ],
      "metadata": {
        "id": "R33Rggcg6riw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# remove extra white spaces\n",
        "tweets['Arabic_processed_tweets']=tweets['Arabic_processed_tweets'].apply(lambda x : re.sub('\\s\\s+', '', x))"
      ],
      "metadata": {
        "id": "FopzGARk8sOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Remove punctuations\n",
        "tweets['Arabic_processed_tweets']=tweets['Arabic_processed_tweets'].apply(lambda x :re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,،-./:;<=>؟?@[\\]^_`{|}~\"\"\"), ' ', x))"
      ],
      "metadata": {
        "id": "PiTrC7E7-5Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['tweet_length']=tweets['Arabic_processed_tweets'].apply(lambda x : len(x))"
      ],
      "metadata": {
        "id": "x929KCt37GHe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " ## Remove cleaned arabic tweets with one or two letters\n",
        "tweets=tweets[tweets['tweet_length']>2]"
      ],
      "metadata": {
        "id": "rMlprIhECrWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.iloc[10:20]"
      ],
      "metadata": {
        "id": "G2i5mZNY2eYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(36,12))\n",
        "tweets['tweet_length'].value_counts().hist(bins=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4vBel2_X2xmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.to_csv('Arabic_cleaned_tweets.csv')"
      ],
      "metadata": {
        "id": "M3rV6bF5RH3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hcs0mywN16Bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**word cloud**"
      ],
      "metadata": {
        "id": "G1xTn8wV1608"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=pd.read_csv('/content/Arabic_cleaned_tweets.csv')\n",
        "tweets.head()"
      ],
      "metadata": {
        "id": "k2MCeAWZQSLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install libaries for arabid word cloud\n",
        "!pip install python-bidi\n",
        "!pip install arabic-reshaper\n",
        "!pip install ar_wordcloud"
      ],
      "metadata": {
        "id": "7S8a6ItFZnLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as font_manager\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from bidi.algorithm import get_display\n",
        "import arabic_reshaper"
      ],
      "metadata": {
        "id": "ljw0ZrvBDiwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets array and prepare list for arabic cloud\n",
        "documents = tweets['Arabic_processed_tweets'].values\n",
        "long_string = ','.join(list(documents))\n",
        "long_string.replace(',', '')"
      ],
      "metadata": {
        "id": "eQ_kiQcyeRdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "G6emtvOUcfnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# arabic cloud\n",
        "from ar_wordcloud import ArabicWordCloud\n",
        "awc = ArabicWordCloud(background_color=\"white\",  width=2000, height=1000)\n",
        "wc = awc.from_text(long_string.replace(',', ''))\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"wordcloud\")\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "U9esI5aywUpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extend stop words\n",
        "\n",
        "def removestopwords(x):\n",
        "  sw = stopwords.words('arabic')\n",
        "  sw.extend(['السلام','عليكم','الي','اللي','اذا','انا','الله','وانا','والله','او', 'ان', 'وانا'])\n",
        "  tokens = nltk.word_tokenize(x)\n",
        "  stopped_tokens = [i for i in tokens if not i in sw]\n",
        "  s=''\n",
        "  for i in stopped_tokens:\n",
        "    s=s+' '+i\n",
        "  return s\n",
        "\n",
        "tweets['Arabic_processed_tweets']=tweets['Arabic_processed_tweets'].apply(lambda x : removestopwords(x))"
      ],
      "metadata": {
        "id": "gLzZ7MONBr6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tweets array and prepare list for arabic cloud\n",
        "documents = tweets['Arabic_processed_tweets'].values\n",
        "long_string = ','.join(list(documents))\n",
        "long_string.replace(',', '')\n",
        "\n",
        "# arabic cloud version 2\n",
        "from ar_wordcloud import ArabicWordCloud\n",
        "awc = ArabicWordCloud(background_color=\"white\",  width=2000, height=1000)\n",
        "wc = awc.from_text(long_string.replace(',', ''))\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.title(\"wordcloud\")\n",
        "plt.imshow(wc)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WqQnVwbcEpgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "  "
      ],
      "metadata": {
        "id": "6M2RfVCSIe10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA**"
      ],
      "metadata": {
        "id": "qQwpMt8TFfRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaMulticore\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF"
      ],
      "metadata": {
        "id": "zvDr45qq3gE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [[word for word in str(document).split()] for document in documents]\n",
        "id2word = corpora.Dictionary(texts)\n",
        "corpus = [id2word.doc2bow(text) for text in texts]"
      ],
      "metadata": {
        "id": "YLH4LPQa3BoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAQRvPetEz2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LDA hyperparameters tuning Alpha and Beta and no of topics**"
      ],
      "metadata": {
        "id": "T5hNyOu1cuxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3, alpha=0.01, eta=0.01):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    alpha =[]\n",
        "    beta=[]\n",
        "    topics=[]\n",
        "    for num_topics in range(start, limit, step):\n",
        "        for i in list(np.arange(0.01, 1, 0.3)):\n",
        "           for j in list(np.arange(0.01, 1, 0.3)):\n",
        "                  model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word, alpha=i, eta=j)\n",
        "                  model_list.append(model)\n",
        "                  coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "                  coherence_values.append(coherencemodel.get_coherence())\n",
        "                  alpha.append(i)\n",
        "                  beta.append(j)\n",
        "                  topics.append(num_topics)\n",
        "\n",
        "    return model_list, coherence_values, alpha, beta, topics"
      ],
      "metadata": {
        "id": "YgEXgaS6WD_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can take a long time to run.\n",
        "model_list, coherence_values, alpha, beta, topics = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=10, step=1)"
      ],
      "metadata": {
        "id": "bXM1RIopXRMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coherence scores\n",
        "for m, cv, a, b in zip(topics, coherence_values, alpha, beta):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4), \" has alpha=\", a, \" and beta=\", b)"
      ],
      "metadata": {
        "id": "zb-RE2seXRhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IqMWxc_IXRmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DaGwURm7XRqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Check best no of topics with default hyper parameters**"
      ],
      "metadata": {
        "id": "HJ9LmpJbc2Qs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
        "    \"\"\"\n",
        "    Compute c_v coherence for various number of topics\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    dictionary : Gensim dictionary\n",
        "    corpus : Gensim corpus\n",
        "    texts : List of input texts\n",
        "    limit : Max num of topics\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    model_list : List of LDA topic models\n",
        "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
        "    \"\"\"\n",
        "    coherence_values = []\n",
        "    model_list = []\n",
        "    \n",
        "    for num_topics in range(start, limit, step):\n",
        "        model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
        "        model_list.append(model)\n",
        "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
        "        coherence_values.append(coherencemodel.get_coherence())\n",
        "      \n",
        "\n",
        "    return model_list, coherence_values"
      ],
      "metadata": {
        "id": "CB5antWQPMjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Can take a long time to run.\n",
        "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=2, limit=10, step=1)"
      ],
      "metadata": {
        "id": "fCpMJfkT16RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show graph\n",
        "limit=10; start=2; step=1;\n",
        "x = range(start, limit, step)\n",
        "plt.plot(x, coherence_values)\n",
        "plt.xlabel(\"Num Topics\")\n",
        "plt.ylabel(\"Coherence score\")\n",
        "plt.legend((\"coherence_values\"), loc='best')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WKI1ugTV16Ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the coherence scores\n",
        "for m, cv in zip(x, coherence_values):\n",
        "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
      ],
      "metadata": {
        "id": "Qz6sahMXVyWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the model and print the topics\n",
        "optimal_model = model_list[5]\n",
        "model_topics = optimal_model.show_topics(formatted=False)\n",
        "print(optimal_model.print_topics(num_words=10))"
      ],
      "metadata": {
        "id": "Lt89SggbTn60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic in each document\n",
        "    for i, row in enumerate(ldamodel[corpus]):\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=tweets['Arabic_processed_tweets'].values.tolist())\n",
        "\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "\n",
        "# Show\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_dominant_topic.head(10)"
      ],
      "metadata": {
        "id": "QNlR6YR9ToDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic['Dominant_Topic'].value_counts()"
      ],
      "metadata": {
        "id": "dN5SIiKqpUYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==6].head()"
      ],
      "metadata": {
        "id": "rep-y_HyToIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==5].head()"
      ],
      "metadata": {
        "id": "tkUpMKIsPuge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==4].head()"
      ],
      "metadata": {
        "id": "VRW2VXH0Puo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==3].head()"
      ],
      "metadata": {
        "id": "7fal91Qxbsxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==2].head()"
      ],
      "metadata": {
        "id": "iS4gO6hSb043"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==1].head()"
      ],
      "metadata": {
        "id": "bAaWBlVJb08W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dominant_topic[df_dominant_topic['Dominant_Topic']==0].head()"
      ],
      "metadata": {
        "id": "mnLnOC85b0_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drVexD2Ib1Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Features Extraction using BERT**\n"
      ],
      "metadata": {
        "id": "HE3IqfJRY_tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.head()"
      ],
      "metadata": {
        "id": "F5j_bgGWPusF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.shape"
      ],
      "metadata": {
        "id": "FG1Y6OcxPuve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "cvsbQfHQPuzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read tweets\n",
        "tweets=pd.read_csv('/content/df_model_input.csv')"
      ],
      "metadata": {
        "id": "ld7FL6jrPu0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets.head()"
      ],
      "metadata": {
        "id": "mmOBy6sToJPi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "WI6mFX-toMhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(device)"
      ],
      "metadata": {
        "id": "z_Vg3hyYITIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=tweets\n",
        "df.head()"
      ],
      "metadata": {
        "id": "x9xcRiclmRj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "6vkWE8zuoDAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "KYpySKG-qzHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.shape, df_val.shape"
      ],
      "metadata": {
        "id": "yksWv3QArQxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to tokenize the text. Remember to define the padding in this way each tokenized sentence will have the same length, and the truncation so if the sentence is too long it will be cut off. The last argument is to return a PyTorch tensor.\n",
        "\n",
        "The result of tokenizing a text will be a dictionary that contains the input_ids , that are the tokens expressed in numbers, and the attention_mask that tells us if the token is or is not a [PAD]."
      ],
      "metadata": {
        "id": "jCMtWtvJr7c-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tokenized_train = tokenizer(df_train[\"Arabic_processed_tweets\"].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n",
        "tokenized_val = tokenizer(df_val[\"Arabic_processed_tweets\"].values.tolist() , padding = True, truncation = True,  return_tensors=\"pt\")\n",
        "\n",
        "print(tokenized_train.keys())\n",
        "\n",
        "#move on device (GPU)\n",
        "tokenized_train = {k:torch.tensor(v).to(device) for k,v in tokenized_train.items()}\n",
        "tokenized_val = {k:torch.tensor(v).to(device) for k,v in tokenized_val.items()}"
      ],
      "metadata": {
        "id": "MUIL3VAYijQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the texts ([CLS]) hidden states by running the model."
      ],
      "metadata": {
        "id": "SCemGV19sSWF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  hidden_train = model(**tokenized_train) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n",
        "  hidden_val = model(**tokenized_val)\n",
        "\n",
        "#get only the [CLS] hidden states\n",
        "cls_train = hidden_train.last_hidden_state[:,0,:]\n",
        "cls_val = hidden_val.last_hidden_state[:,0,:]"
      ],
      "metadata": {
        "id": "T3QJmy8TrqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = cls_train.to(\"cpu\")\n",
        "y_train = df_train[\"sentiment\"]\n",
        "\n",
        "x_val = cls_val.to(\"cpu\")\n",
        "y_val = df_val[\"sentiment\"]\n",
        "\n",
        "print(x_train.shape, y_train.shape, x_val.shape, y_val.shape)"
      ],
      "metadata": {
        "id": "IQ6uZz8tJkrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0-cwaV6BZQ7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('df_model_input.csv')"
      ],
      "metadata": {
        "id": "Zxw2JFB0sXfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/df_model_input.csv')\n",
        "df=df[['Arabic_processed_tweets','sentiment']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CH8TRArut4Nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3J5mqsz7ZRnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "id": "EFI0A6C1HZmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "# Chooses the support vector machine algorithm for our classifier\n",
        "clf = svm.SVC(kernel = \"linear\")"
      ],
      "metadata": {
        "id": "IpNB04X3H0mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_trained=clf.fit(x_train, y_train)\n",
        "# Scoring the classifier\n",
        "clf_trained.score(x_train, y_train)"
      ],
      "metadata": {
        "id": "GWWRPGR8WDSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary functions\n",
        "from sklearn import metrics\n",
        "#extract the predictions of the model\n",
        "test_pred_svm = clf_trained.predict(x_val)\n",
        "#print the classification report\n",
        "print (metrics.classification_report(y_val, test_pred_svm))"
      ],
      "metadata": {
        "id": "JstmFMa4WSCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AyBtciSwZ-1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using MARBERT Features Extraction with SVM**"
      ],
      "metadata": {
        "id": "Gri0MX5maMIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "model = pipeline('text-classification', model='Ammar-alhaj-ali/arabic-MARBERT-sentiment')\n",
        "sentences = ['لقد استمتعت بالحفلة', 'خدمة المطعم كانت محبطة']\n",
        "model(sentences)"
      ],
      "metadata": {
        "id": "u16C8WkPWtrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "device = torch.device('cpu')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\")\n",
        "model = AutoModel.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\").to(device)"
      ],
      "metadata": {
        "id": "vbpRFPiuWDtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].value_counts()"
      ],
      "metadata": {
        "id": "qQQUDPg7WHyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train, df_val = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "alTUQmZDWNzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_train_1 = tokenizer(df_train[\"Arabic_processed_tweets\"].values.tolist(), padding = True, truncation = True, return_tensors=\"pt\")\n",
        "tokenized_val_1 = tokenizer(df_val[\"Arabic_processed_tweets\"].values.tolist() , padding = True, truncation = True,  return_tensors=\"pt\")\n",
        "\n",
        "print(tokenized_train_1.keys())\n",
        "\n",
        "#move on device (GPU)\n",
        "tokenized_train_1 = {k:torch.tensor(v).to(device) for k,v in tokenized_train_1.items()}\n",
        "tokenized_val_1= {k:torch.tensor(v).to(device) for k,v in tokenized_val_1.items()}"
      ],
      "metadata": {
        "id": "pDNTv_fpWSiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "  hidden_train_1 = model(**tokenized_train_1) #dim : [batch_size(nr_sentences), tokens, emb_dim]\n",
        "  hidden_val_1 = model(**tokenized_val_1)\n",
        "\n",
        "#get only the [CLS] hidden states\n",
        "cls_train_1 = hidden_train_1.last_hidden_state[:,0,:]\n",
        "cls_val_1 = hidden_val_1.last_hidden_state[:,0,:]"
      ],
      "metadata": {
        "id": "U8LuOXZDWfkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_1 = cls_train_1.to(\"cpu\")\n",
        "y_train_1 = df_train[\"sentiment\"]\n",
        "\n",
        "x_val_1 = cls_val_1.to(\"cpu\")\n",
        "y_val_1 = df_val[\"sentiment\"]\n",
        "\n",
        "print(x_train_1.shape, y_train_1.shape, x_val_1.shape, y_val_1.shape)"
      ],
      "metadata": {
        "id": "H7jQhUnGXBby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "id": "1l6CH8VIXNao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "# Chooses the support vector machine algorithm for our classifier\n",
        "clf = svm.SVC(kernel = \"linear\")"
      ],
      "metadata": {
        "id": "fxnYsYjaXgl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf_trained=clf.fit(x_train_1, y_train_1)\n",
        "# Scoring the classifier\n",
        "clf_trained.score(x_train_1, y_train_1)"
      ],
      "metadata": {
        "id": "phZBxT3DXljt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import the necessary functions\n",
        "from sklearn import metrics\n",
        "#extract the predictions of the model\n",
        "test_pred_svm_1 = clf_trained.predict(x_val_1)\n",
        "#print the classification report\n",
        "print (metrics.classification_report(y_val_1, test_pred_svm_1))"
      ],
      "metadata": {
        "id": "JW46FGTuX14c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jd-KRPj-YAjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sLivHpEQetE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MARBERT**"
      ],
      "metadata": {
        "id": "XxfI_bjke3fY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/df_model_input.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "RYB3VV_t9lR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[['Arabic_processed_tweets', 'sentiment']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "qSQxl5um9r65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ],
      "metadata": {
        "id": "2cwXmYAqAF2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df.Arabic_processed_tweets.values\n",
        "labels = df.sentiment.values"
      ],
      "metadata": {
        "id": "OcUShuw9AF5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('UBC-NLP/MARBERT')"
      ],
      "metadata": {
        "id": "EK7JCAiwAF9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_rand_sentence():\n",
        "  '''Displays the tokens and respective IDs of a random text sample'''\n",
        "  index = random.randint(0, len(text)-1)\n",
        "  table = np.array([tokenizer.tokenize(text[index]), \n",
        "                    tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text[index]))]).T\n",
        "  print(tabulate(table,\n",
        "                 headers = ['Token IDs', 'Tokens'],\n",
        "                 tablefmt = 'fancy_grid'))\n",
        "\n",
        "print_rand_sentence()"
      ],
      "metadata": {
        "id": "soU1_LpuAF_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "cGaEYlnZAGCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_rand_sentence_encoding():\n",
        "  '''Displays tokens, token IDs and attention mask of a random text sample'''\n",
        "  index = random.randint(0, len(text) - 1)\n",
        "  tokens = tokenizer.tokenize(tokenizer.decode(token_id[index]))\n",
        "  token_ids = [i.numpy() for i in token_id[index]]\n",
        "  attention = [i.numpy() for i in attention_masks[index]]\n",
        "\n",
        "  table = np.array([tokens, token_ids, attention])\n",
        "  print(tabulate(table, \n",
        "                 #headers = ['Tokens', 'Token IDs', 'Attention Mask'],\n",
        "                 tablefmt = 'fancy_grid'\n",
        "                 ))\n",
        "\n",
        "print_rand_sentence_encoding()"
      ],
      "metadata": {
        "id": "TxIyigoaAGFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "5XS4fk7DAGIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity"
      ],
      "metadata": {
        "id": "LWke7xSbPIig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'UBC-NLP/MARBERT',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "aCy_hC_bPIl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "    \n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "        \n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "    print('\\t - validation F1-Score: {:.4f}\\n'.format(2*(sum(val_precision)/len(val_precision))*(sum(val_recall)/len(val_recall))/((sum(val_precision)/len(val_precision))+(sum(val_recall)/len(val_recall)))))"
      ],
      "metadata": {
        "id": "SJjWmZpiPIoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zeqbHtYyPIra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ammar-alhaj-ali/arabic-MARBERT-sentiment"
      ],
      "metadata": {
        "id": "SRpYvrklDm5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('Ammar-alhaj-ali/arabic-MARBERT-sentiment')"
      ],
      "metadata": {
        "id": "AUqy6smOPIuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "ctI1b6gtPIw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "L7J-H2X6PIzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - specificity = TN / (TN + FP)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_specificity = tn / (tn + fp) if (tn + fp) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_specificity"
      ],
      "metadata": {
        "id": "iTCI0M_1PI2l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'Ammar-alhaj-ali/arabic-MARBERT-sentiment',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "5cRnJY_EPI6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 2\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "    \n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "        \n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "    print('\\t - validation F1-Score: {:.4f}\\n'.format(2*(sum(val_precision)/len(val_precision))*(sum(val_recall)/len(val_recall))/((sum(val_precision)/len(val_precision))+(sum(val_recall)/len(val_recall)))))"
      ],
      "metadata": {
        "id": "ph8GJzZzDT77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zrV64otrDUcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UBC-NLP/MARBERT Max_length 64 and no of epochs 4"
      ],
      "metadata": {
        "id": "VPAxFavnGPt5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('UBC-NLP/MARBERT')\n",
        "\n",
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 64,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'UBC-NLP/MARBERT',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 4\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "    \n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "        \n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "    print('\\t - validation F1-Score: {:.4f}\\n'.format(2*(sum(val_precision)/len(val_precision))*(sum(val_recall)/len(val_recall))/((sum(val_precision)/len(val_precision))+(sum(val_recall)/len(val_recall)))))"
      ],
      "metadata": {
        "id": "QOBVql0FDUg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0sfsJMmqDUkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ammar-alhaj-ali/arabic-MARBERT-sentiment Max length =64 epochs 4 and truncation is true"
      ],
      "metadata": {
        "id": "l5zJink5H2U3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize Bert tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('Ammar-alhaj-ali/arabic-MARBERT-sentiment')\n",
        "\n",
        "token_id = []\n",
        "attention_masks = []\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 128,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt',\n",
        "                        truncation=True\n",
        "                   )\n",
        "\n",
        "\n",
        "for sample in text:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids']) \n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Indices of the train and validation splits stratified by labels\n",
        "train_idx, val_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True,\n",
        "    stratify = labels)\n",
        "\n",
        "# Train and validation sets\n",
        "train_set = TensorDataset(token_id[train_idx], \n",
        "                          attention_masks[train_idx], \n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx], \n",
        "                        attention_masks[val_idx], \n",
        "                        labels[val_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "\n",
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'Ammar-alhaj-ali/arabic-MARBERT-sentiment',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(), \n",
        "                              lr = 2e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()\n",
        "\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 4\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "    \n",
        "    # ========== Training ==========\n",
        "    \n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "    \n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids, \n",
        "                             token_type_ids = None, \n",
        "                             attention_mask = b_input_mask, \n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_specificity = []\n",
        "    \n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids, \n",
        "                              token_type_ids = None, \n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_specificity = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_specificity != 'nan': val_specificity.append(b_specificity)\n",
        "        \n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy: {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall: {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation Specificity: {:.4f}\\n'.format(sum(val_specificity)/len(val_specificity)) if len(val_specificity)>0 else '\\t - Validation Specificity: NaN')\n",
        "    print('\\t - validation F1-Score: {:.4f}\\n'.format(2*(sum(val_precision)/len(val_precision))*(sum(val_recall)/len(val_recall))/((sum(val_precision)/len(val_precision))+(sum(val_recall)/len(val_recall)))))"
      ],
      "metadata": {
        "id": "w-Kbft2LDUnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nLsy4WvTDUrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8VjC0wmSHxfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4liaqIvUHxiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4D31nmrHxlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z42s3r2iHxoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[['Arabic_processed_tweets','sentiment']]\n",
        "df.rename(columns={\"Arabic_processed_tweets\": \"text\", \"sentiment\": \"label\"}, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "SkiZjFvunSxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "azZyvprCEuyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModel.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\")"
      ],
      "metadata": {
        "id": "OJMP0UTketIA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define preprocessing util function\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "  \n",
        "\n",
        "    # Normalize unicode encoding\n",
        "    text = unicodedata.normalize('NFC', text)\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    #Remove URLs\n",
        "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '<URL>', text)\n",
        "\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "FKer-leYjTwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install emoji"
      ],
      "metadata": {
        "id": "nAbD2Y3tj4fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "import emoji\n",
        "import unicodedata\n",
        "def preprocessing_for_bert(data, version=\"mini\", text_preprocessing_fn = text_preprocessing ):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\") if version == \"mini\" else AutoTokenizer.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\")\n",
        "\n",
        "    # For every sentence...\n",
        "    for i,sent in enumerate(data):\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing_fn(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            padding='max_length',        # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True,     # Return attention mask\n",
        "            truncation = True \n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "mvZBbleYjc7b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = df\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X = train_df.text.values\n",
        "y = train_df.label.values\n",
        "\n",
        "# The train val split is used by the DL approach but not classical ML\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.2, random_state=2020)"
      ],
      "metadata": {
        "id": "Ewgdg_TJkZ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "3iRobk30kydc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN =  280\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)"
      ],
      "metadata": {
        "id": "J-B5-LXOj9xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "0dcM95xek20o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False, version=\"mini\"):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in = 256 if version == \"mini\" else 256\n",
        "        H, D_out = 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = AutoModel.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\") if version == \"mini\" else AutoModel.from_pretrained(\"Ammar-alhaj-ali/arabic-MARBERT-sentiment\")\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveMaxPool2d((D_in,256)),\n",
        "            nn.Linear(256, H),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask, )\n",
        "        \n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "z_WIeOqSlBNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TIhYjQnhzZpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from torch.optim import SparseAdam, Adam\n",
        "def initialize_model(epochs=4, version=\"mini\"):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False, version=version)\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(params=list(bert_classifier.parameters()),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ],
      "metadata": {
        "id": "YSXtBPTslBQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "            \n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "    \n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ],
      "metadata": {
        "id": "w1Dj9i2nlBTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.memory_summary(device=None, abbreviated=False)"
      ],
      "metadata": {
        "id": "A6ghYlyGq8Nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "set_seed(42) \n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ],
      "metadata": {
        "id": "dZra_q-olBWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hidden_states(batch):\n",
        "    inputs = {k:v.to(device) for k,v in batch.items()\n",
        "             if k in tokenizer.model_input_names}\n",
        "    with torch.no_grad():\n",
        "        last_hidden_state = model(**inputs).last_hidden_state\n",
        "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
      ],
      "metadata": {
        "id": "A-mIik4te0p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch, padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "8Q2zowGEfYgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].value_counts()"
      ],
      "metadata": {
        "id": "Z6cacRSnnkcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_ds, df_r = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
        "val_ds, test_ds = train_test_split(df, test_size=0.5, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "4pAmiIOioGLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_dict = {'Negative':0, 'Positive':1}"
      ],
      "metadata": {
        "id": "YksyIiWsFEg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = dict(zip(emotion_dict.values(),emotion_dict.keys()))"
      ],
      "metadata": {
        "id": "nELEIh6vMiwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p[0]"
      ],
      "metadata": {
        "id": "Ygdo9d7iMlCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "BiTUmmmwXqgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(\"text\", data_files=\"/content/df_model_input.txt\")['train']"
      ],
      "metadata": {
        "id": "mI8TOk71XvOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[0]['text'].split(',')[0]"
      ],
      "metadata": {
        "id": "BBroNRo3ZVuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "    after_process = data['text'].split(',')\n",
        "    data['text'] = after_process[1]\n",
        "    data['label'] = emotion_dict[after_process[0]]\n",
        "    return data"
      ],
      "metadata": {
        "id": "GyWUnIsBX_GY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds =  train_ds.map(lambda x: split_data(x))\n",
        "test_ds = test_ds.map(lambda x: split_data(x))\n",
        "val_ds = val_ds.map(lambda x: split_data(x))"
      ],
      "metadata": {
        "id": "LJ30vWZ4Z5Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "\n",
        "# creating dummy targets (float values)\n",
        "targets_data = [random.random() for i in range(10)]\n",
        "\n",
        "# creating DataFrame from targets_data\n",
        "targets_df = pd.DataFrame(data=targets_data)\n",
        "targets_df.columns = ['targets']\n",
        "\n",
        "# creating tensor from targets_df \n",
        "torch_tensor = torch.tensor(targets_df['targets'].values)\n",
        "\n",
        "# printing out result\n",
        "print(torch_tensor)"
      ],
      "metadata": {
        "id": "Cp8tFd6aOt2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_encoded =  train_ds.map(tokenize, batched=True, batch_size=None)\n",
        "test_ds_encoded =  test_ds.map(tokenize, batched=True, batch_size=None)\n",
        "val_ds_encoded =  val_ds.map(tokenize, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "gwBK3nhUnNey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_ds_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "val_ds_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "_h_tti2Ee_S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d=pd.read_csv('/content/train.txt')"
      ],
      "metadata": {
        "id": "QGqu6v29fFJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d.head()"
      ],
      "metadata": {
        "id": "Jmj4bGybk-Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_dict = {'surprise':0, 'love':1 , 'joy':2 , 'fear': 3, 'sadness': 4, 'anger':5}"
      ],
      "metadata": {
        "id": "GISSeLYolCiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "n-v7je2_mMSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "yFbwkIaHmOph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_train_ds = load_dataset(df)['train']"
      ],
      "metadata": {
        "id": "3RBU3rXclbWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "before_train_ds"
      ],
      "metadata": {
        "id": "SaMCDVeKlkdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " emotion_dict[before_train_ds[100]['text'].split(';')[1]]"
      ],
      "metadata": {
        "id": "I8jow8xImY5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "    after_process = data['text'].split(';')\n",
        "    data['text'] = after_process[0]\n",
        "    data['label'] = emotion_dict[after_process[1]]\n",
        "    return data\n",
        "\n",
        "train_ds =  before_train_ds.map(lambda x: split_data(x))"
      ],
      "metadata": {
        "id": "T-3_q2CkmdR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds"
      ],
      "metadata": {
        "id": "BlxNs-YRmknv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds.set_format(type=\"pandas\")\n",
        "df = train_ds[:]\n",
        "df"
      ],
      "metadata": {
        "id": "5_RNIHdRml0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p = dict(zip(emotion_dict.values(),emotion_dict.keys()))"
      ],
      "metadata": {
        "id": "ryxqJVkZmtN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['label_text'] =df['label'].apply(lambda x: p[x])"
      ],
      "metadata": {
        "id": "7WWA5AvpmyD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "J5XOlBD-m0St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_encoded =  train_ds.map(tokenize, batched=True, batch_size=None)"
      ],
      "metadata": {
        "id": "2GRnwEqTm19_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(\"text\", skiprow=1, data_files=\"/content/df_model_input.csv\")['train']"
      ],
      "metadata": {
        "id": "ZDgJWOlTm992"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "rb8U3l7Xrx-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[1:, :]"
      ],
      "metadata": {
        "id": "rvklRuvstPvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[1]"
      ],
      "metadata": {
        "id": "0sXjHyDTtqkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "    after_process = data['text'].split(',')\n",
        "    data['text'] = after_process[0]\n",
        "    data['label'] = after_process[1]\n",
        "    return data"
      ],
      "metadata": {
        "id": "BwJ5NhCIr7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df =  df.map(lambda x: split_data(x))"
      ],
      "metadata": {
        "id": "RJ9b8XR9sPTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GrKT_8vesXlk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}